{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "45c594a69f314882a67ff1e97a97c509": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d04c78c788424818b7e2b69429e00209",
              "IPY_MODEL_d97c0346d514463a83d975480d88f410",
              "IPY_MODEL_c0e5d2638b254eb2a30d91a77c51edd7"
            ],
            "layout": "IPY_MODEL_871f829dbd8c4ca8a25995789f0992c8"
          }
        },
        "d04c78c788424818b7e2b69429e00209": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d373927054c94baebabd9de8ef704c2f",
            "placeholder": "​",
            "style": "IPY_MODEL_6b5ed0ac8b6f44cdbb6fffa5889bfd88",
            "value": "Map: 100%"
          }
        },
        "d97c0346d514463a83d975480d88f410": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad4ecd2a4a4340b2bb80be60b6bffab5",
            "max": 1356,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cb39eafd94c045c8b7f7eab37757381f",
            "value": 1356
          }
        },
        "c0e5d2638b254eb2a30d91a77c51edd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8dfbf381b0814bdd817822262f55fcca",
            "placeholder": "​",
            "style": "IPY_MODEL_ddc0aa2124fe4d0291128f6bdc0448dc",
            "value": " 1356/1356 [00:03&lt;00:00, 405.82 examples/s]"
          }
        },
        "871f829dbd8c4ca8a25995789f0992c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d373927054c94baebabd9de8ef704c2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b5ed0ac8b6f44cdbb6fffa5889bfd88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ad4ecd2a4a4340b2bb80be60b6bffab5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb39eafd94c045c8b7f7eab37757381f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8dfbf381b0814bdd817822262f55fcca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ddc0aa2124fe4d0291128f6bdc0448dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0b8bc6bea47542b889e11dfb4eed41ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0c839f99938943238504d4c829518620",
              "IPY_MODEL_81876db14da34448983353cfcc912698",
              "IPY_MODEL_30145beb2e084111a4056faad65b254b"
            ],
            "layout": "IPY_MODEL_cc6f476a597a469e8a1647d39375c210"
          }
        },
        "0c839f99938943238504d4c829518620": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d96bfa0b8714647bb0530377e69c06a",
            "placeholder": "​",
            "style": "IPY_MODEL_de7c605a1b5d42249084723ce170295c",
            "value": "Downloading builder script: 100%"
          }
        },
        "81876db14da34448983353cfcc912698": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41f6ede7d0a84caeb7b47e050e640d5c",
            "max": 4203,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b0a5f90e8b5f4857995f8dddf375739f",
            "value": 4203
          }
        },
        "30145beb2e084111a4056faad65b254b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d279834f0cad414f87dd75654af3dace",
            "placeholder": "​",
            "style": "IPY_MODEL_08b6b13ec7c94e72b28be209e29d5414",
            "value": " 4.20k/4.20k [00:00&lt;00:00, 184kB/s]"
          }
        },
        "cc6f476a597a469e8a1647d39375c210": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d96bfa0b8714647bb0530377e69c06a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de7c605a1b5d42249084723ce170295c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41f6ede7d0a84caeb7b47e050e640d5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0a5f90e8b5f4857995f8dddf375739f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d279834f0cad414f87dd75654af3dace": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08b6b13ec7c94e72b28be209e29d5414": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Purab-Gaonkar/Sentimental-Analysis/blob/main/Untitled7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tCbSCJkQqTg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9bcb9d4-5111-4b82-de75-f80a26d38ceb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Enter YouTube Video URL: https://www.youtube.com/watch?v=a4NJNdHqs_I&pp=ygUFbWtiaGQ%3D\n",
            "Video ID: a4NJNdHqs_I\n",
            "Channel ID: UCBJycsmduvYEL83R_U4JriQ\n",
            "Fetching Comments...\n",
            "\n",
            "First 5 comments (English only, excluding uploader):\n",
            "1: Finally someone from US has highlighted other brands like OnePlus, Vivo who are really trying to give a whole package.\n",
            "2: This is why I&#39;ll always keep my S10+ even though I&#39;m on the S24....talk about peak\n",
            "3: I spent $1500 on a z fold 6 so that&#39;s worth it in my opinion. Lol\n",
            "4: Samsung lost me way back with the removal of the IR Blaster,removable battery,SD card, ect.\n",
            "5: Samsung : many features none of them work perfectly\n",
            "Comments stored in Google Drive at: /content/drive/My Drive/ytcomments.txt\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from googleapiclient.discovery import build\n",
        "from urllib.parse import urlparse, parse_qs\n",
        "from google.colab import files  # For downloading the file\n",
        "from google.colab import drive  # For accessing Google Drive\n",
        "\n",
        "API_KEY = 'AIzaSyBtJjniAVMDaQN3OaocZpP7MyAVBgXPl2Q'  # Replace with your actual API key\n",
        "\n",
        "youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Function to extract video ID from YouTube URL\n",
        "def get_video_id(url):\n",
        "    query = urlparse(url)\n",
        "    if query.hostname == 'youtu.be':\n",
        "        return query.path[1:]\n",
        "    elif query.hostname in ('www.youtube.com', 'youtube.com'):\n",
        "        if query.path == '/watch':\n",
        "            p = parse_qs(query.query)\n",
        "            return p['v'][0]\n",
        "        elif query.path[:7] == '/embed/':\n",
        "            return query.path.split('/')[2]\n",
        "        elif query.path[:3] == '/v/':\n",
        "            return query.path.split('/')[2]\n",
        "    return None\n",
        "\n",
        "# Get video URL and ID\n",
        "video_url = input('Enter YouTube Video URL: ')\n",
        "video_id = get_video_id(video_url)\n",
        "print(\"Video ID:\", video_id)\n",
        "\n",
        "# Get uploader's channel ID\n",
        "video_response = youtube.videos().list(part='snippet', id=video_id).execute()\n",
        "video_snippet = video_response['items'][0]['snippet']\n",
        "uploader_channel_id = video_snippet['channelId']\n",
        "print(\"Channel ID:\", uploader_channel_id)\n",
        "\n",
        "# Function to check if a comment is in English\n",
        "def is_english(comment_text):\n",
        "    return re.match(r'^[\\x00-\\x7F]+$', comment_text) is not None\n",
        "\n",
        "# Fetch comments\n",
        "print(\"Fetching Comments...\")\n",
        "comments = []\n",
        "nextPageToken = None\n",
        "\n",
        "try:\n",
        "    while len(comments) < 600:\n",
        "        request = youtube.commentThreads().list(\n",
        "            part='snippet',\n",
        "            videoId=video_id,\n",
        "            maxResults=100,\n",
        "            pageToken=nextPageToken\n",
        "        )\n",
        "        response = request.execute()\n",
        "\n",
        "        for item in response['items']:\n",
        "            comment = item['snippet']['topLevelComment']['snippet']\n",
        "            comment_text = comment['textDisplay']\n",
        "            if comment['authorChannelId']['value'] != uploader_channel_id and is_english(comment_text):\n",
        "                comments.append(comment_text)\n",
        "\n",
        "        nextPageToken = response.get('nextPageToken')\n",
        "        if not nextPageToken:\n",
        "            break\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Error occurred:\", str(e))\n",
        "\n",
        "# Display the first 5 fetched comments\n",
        "if comments:\n",
        "    print(\"\\nFirst 5 comments (English only, excluding uploader):\")\n",
        "    for i, comment in enumerate(comments[:5], start=1):\n",
        "        print(f\"{i}: {comment}\")\n",
        "else:\n",
        "    print(\"No comments fetched.\")\n",
        "\n",
        "# Define the path in Google Drive where you want to save the file\n",
        "drive_path = '/content/drive/My Drive/ytcomments.txt'\n",
        "\n",
        "# Save comments to a text file in Google Drive\n",
        "with open(drive_path, 'a', encoding=\"utf-8\") as f:\n",
        "    for comment in comments:\n",
        "        f.write(comment + \"\\n\")  # Write each comment to a new line\n",
        "\n",
        "print(\"Comments stored in Google Drive at:\", drive_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "awo5Hp0-UA64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import emoji\n",
        "import re\n",
        "from google.colab import drive  # To access Google Drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Download the VADER lexicon from NLTK\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Initialize the SentimentIntensityAnalyzer from VADER\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Define the key points related to mobile phones\n",
        "key_points = ['camera', 'battery', 'screen', 'performance', 'price', 'design', 'audio', 'build quality', 'display', 'storage']\n",
        "\n",
        "# Function to preprocess the data\n",
        "def preprocess(comment):\n",
        "    # Convert the comment to lowercase and strip unnecessary spaces\n",
        "    comment = comment.lower().strip()\n",
        "\n",
        "    # Remove URLs\n",
        "    comment = re.sub(r'http\\S+', '', comment)\n",
        "\n",
        "    # Remove mentions (e.g., @username)\n",
        "    comment = re.sub(r'@\\w+', '', comment)\n",
        "\n",
        "    # Remove hashtags\n",
        "    comment = re.sub(r'#\\w+', '', comment)\n",
        "\n",
        "    # Remove emojis using regex (emoji characters are typically outside basic Unicode ranges)\n",
        "    comment = re.sub(r'[^\\x00-\\x7F]+', '', comment)  # Removes non-ASCII characters (emojis)\n",
        "\n",
        "    # Remove all punctuation\n",
        "    comment = re.sub(r'[^\\w\\s]', '', comment)  # Keeps only words and spaces (removes punctuation)\n",
        "\n",
        "    # Count remaining text characters\n",
        "    text_characters = len(comment.replace(\" \", \"\"))\n",
        "\n",
        "    # If the comment is not empty after cleaning, return it\n",
        "    if text_characters > 0:\n",
        "        return comment\n",
        "    return None\n",
        "\n",
        "# Function to classify sentiment using VADER lexicon and return numeric labels\n",
        "def lexicon_based_sentiment(comment):\n",
        "    # Get sentiment scores using VADER\n",
        "    sentiment_score = sia.polarity_scores(comment)\n",
        "\n",
        "    # Classify sentiment based on compound score and return numeric labels\n",
        "    compound_score = sentiment_score['compound']\n",
        "    if compound_score >= 0.05:\n",
        "        return 1  # Positive sentiment\n",
        "    elif compound_score <= -0.05:\n",
        "        return 0  # Negative sentiment\n",
        "    else:\n",
        "        return 2  # Neutral sentiment\n",
        "\n",
        "# Function to detect sentiment for specific key points\n",
        "def detect_key_point_sentiment(comment, key_points):\n",
        "    sentiment_labels = {}\n",
        "    for key_point in key_points:\n",
        "        # Check if the key point is mentioned in the comment\n",
        "        if key_point in comment:\n",
        "            sentiment_labels[key_point] = lexicon_based_sentiment(comment)\n",
        "    return sentiment_labels\n",
        "\n",
        "# Define paths for Google Drive (adjust if needed)\n",
        "input_file_path = '/content/drive/My Drive/ytcomments.txt'  # Update with your actual file path\n",
        "output_file_path = '/content/drive/My Drive/sentiment_data2.csv'  # Path for saving the CSV\n",
        "\n",
        "# Read comments from the file in Google Drive\n",
        "with open(input_file_path, 'r', encoding='utf-8') as f:\n",
        "    comments = f.readlines()\n",
        "\n",
        "# Preprocess comments and write to CSV in Google Drive\n",
        "with open(output_file_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    fieldnames = ['comment', 'sentiment_label'] + key_points  # Include the key points as columns\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "    # Write header only if the file is empty (avoid writing header repeatedly)\n",
        "    if csvfile.tell() == 0:\n",
        "        writer.writeheader()\n",
        "\n",
        "    for comment in comments:\n",
        "        processed_comment = preprocess(comment)\n",
        "        if processed_comment:\n",
        "            # Get overall sentiment using lexicon\n",
        "            overall_sentiment = lexicon_based_sentiment(processed_comment)\n",
        "\n",
        "            # Get sentiment labels for each key point\n",
        "            key_point_sentiments = detect_key_point_sentiment(processed_comment, key_points)\n",
        "\n",
        "            # Merge overall sentiment with key point sentiment labels\n",
        "            row = {'comment': processed_comment, 'sentiment_label': overall_sentiment}\n",
        "            row.update(key_point_sentiments)  # Add the key point sentiments to the row\n",
        "\n",
        "            # Write the row to the CSV\n",
        "            writer.writerow(row)\n",
        "\n",
        "print(\"Sentiment data stored in Google Drive at:\", output_file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSjBOnKCSRuj",
        "outputId": "d4e977f3-85cd-4648-e388-0408bc9dc7a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment data stored in Google Drive at: /content/drive/My Drive/sentiment_data2.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sT0cKQPwlnel",
        "outputId": "46bad11b-d6cb-41ee-943f-d87d0043092d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.3.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.3.0-py3-none-any.whl (484 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m484.9/484.9 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-3.3.0 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kgt9Nic98zK",
        "outputId": "3d5dde7e-cbd8-4ecc-b133-aa0c152e4e37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.10.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.17.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.12)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from evaluate import load\n",
        "import os\n",
        "from google.colab import drive  # For mounting Google Drive\n",
        "\n",
        "# Mount Google Drive to save models there\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define model and tokenizer path on Google Drive\n",
        "drive_model_path = \"/content/drive/MyDrive/sentiment_model\"  # Update this path according to your folder structure\n",
        "\n",
        "# Ensure the code runs on GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Check CUDA availability\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"CUDA is not available. Using CPU.\")\n",
        "else:\n",
        "    print(\"CUDA is available. Using GPU.\")\n",
        "\n",
        "# Load pre-trained tokenizer and model\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3)  # Set num_labels as 3 for sentiment classification (positive, negative, neutral)\n",
        "\n",
        "# Move model to GPU if available\n",
        "model = model.to(device)\n",
        "\n",
        "# Load data from CSV (assuming your data is in 'sentiment_data2.csv' stored in Google Drive)\n",
        "data_path = \"/content/drive/MyDrive/sentiment_data2.csv\"  # Update this path with your file's location in Google Drive\n",
        "df = pd.read_csv(data_path)\n",
        "\n",
        "# Ensure the dataset has a 'label' column for sentiment classification (e.g., 0, 1, 2 for negative, neutral, positive)\n",
        "# If the 'label' column is missing, you can create one. For example:\n",
        "# df['label'] = df['sentiment_column'].map({'negative': 0, 'neutral': 1, 'positive': 2})\n",
        "\n",
        "# Prepare the dataset\n",
        "def preprocess_function(examples):\n",
        "    # Tokenize the text and ensure labels are included\n",
        "    return tokenizer(examples['comment'], padding='max_length', truncation=True, max_length=512)\n",
        "\n",
        "# Convert the pandas DataFrame into Hugging Face Dataset object\n",
        "dataset = Dataset.from_pandas(df)\n",
        "\n",
        "# Preprocess the dataset\n",
        "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# Split the dataset into train and test sets using Hugging Face's train_test_split\n",
        "split_dataset = tokenized_dataset.train_test_split(test_size=0.2)\n",
        "\n",
        "# Extract train and test datasets\n",
        "train_dataset = split_dataset['train']\n",
        "eval_dataset = split_dataset['test']\n",
        "\n",
        "# Ensure the datasets contain the 'labels' column\n",
        "train_dataset = train_dataset.rename_column(\"sentiment_label\", \"labels\")\n",
        "eval_dataset = eval_dataset.rename_column(\"sentiment_label\", \"labels\")\n",
        "\n",
        "# Define accuracy metric\n",
        "metric = load('accuracy')\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',              # Output directory for saving model checkpoints\n",
        "    eval_strategy='epoch',               # Evaluate after each epoch (fixing the deprecation warning)\n",
        "    save_strategy='epoch',              # Save model after each epoch\n",
        "    learning_rate=2e-5,                 # Learning rate\n",
        "    per_device_train_batch_size=16,     # Batch size for training\n",
        "    per_device_eval_batch_size=16,      # Batch size for evaluation\n",
        "    num_train_epochs=3,                 # Number of epochs\n",
        "    weight_decay=0.01,                  # Weight decay for regularization\n",
        "    logging_dir='./logs',               # Directory for logs\n",
        "    save_total_limit=2,                 # Limit the number of saved checkpoints\n",
        "    load_best_model_at_end=True,        # Load the best model at the end\n",
        "    metric_for_best_model=\"accuracy\",  # Metric to use for saving the best model\n",
        "    no_cuda=False,                      # Ensure CUDA is used if available\n",
        "    report_to=\"none\"                    # Disable W&B logging\n",
        ")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the trained model and tokenizer to Google Drive\n",
        "os.makedirs(drive_model_path, exist_ok=True)\n",
        "\n",
        "model.save_pretrained(drive_model_path)\n",
        "tokenizer.save_pretrained(drive_model_path)\n",
        "\n",
        "print(f\"Model trained and saved successfully at {drive_model_path}!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278,
          "referenced_widgets": [
            "45c594a69f314882a67ff1e97a97c509",
            "d04c78c788424818b7e2b69429e00209",
            "d97c0346d514463a83d975480d88f410",
            "c0e5d2638b254eb2a30d91a77c51edd7",
            "871f829dbd8c4ca8a25995789f0992c8",
            "d373927054c94baebabd9de8ef704c2f",
            "6b5ed0ac8b6f44cdbb6fffa5889bfd88",
            "ad4ecd2a4a4340b2bb80be60b6bffab5",
            "cb39eafd94c045c8b7f7eab37757381f",
            "8dfbf381b0814bdd817822262f55fcca",
            "ddc0aa2124fe4d0291128f6bdc0448dc",
            "0b8bc6bea47542b889e11dfb4eed41ea",
            "0c839f99938943238504d4c829518620",
            "81876db14da34448983353cfcc912698",
            "30145beb2e084111a4056faad65b254b",
            "cc6f476a597a469e8a1647d39375c210",
            "2d96bfa0b8714647bb0530377e69c06a",
            "de7c605a1b5d42249084723ce170295c",
            "41f6ede7d0a84caeb7b47e050e640d5c",
            "b0a5f90e8b5f4857995f8dddf375739f",
            "d279834f0cad414f87dd75654af3dace",
            "08b6b13ec7c94e72b28be209e29d5414"
          ]
        },
        "id": "7JwYqujNSjCr",
        "outputId": "a72e686a-4449-44dc-9c4d-7ac1d38c43ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Using device: cuda\n",
            "CUDA is available. Using GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1356 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "45c594a69f314882a67ff1e97a97c509"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0b8bc6bea47542b889e11dfb4eed41ea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='116' max='204' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [116/204 01:26 < 01:06, 1.32 it/s, Epoch 1.69/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.925504</td>\n",
              "      <td>0.558824</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from faker import Faker\n",
        "import random\n",
        "import re\n",
        "\n",
        "# Define the key points related to mobile phones\n",
        "key_points = ['camera', 'battery', 'screen', 'performance', 'price', 'design', 'audio', 'build quality', 'display', 'storage']\n",
        "\n",
        "# Initialize Faker and VADER\n",
        "fake = Faker()\n",
        "nltk.download('vader_lexicon')\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Function to preprocess the data\n",
        "def preprocess(comment):\n",
        "    comment = comment.lower().strip()\n",
        "    comment = re.sub(r'http\\S+', '', comment)  # Remove URLs\n",
        "    comment = re.sub(r'@\\w+', '', comment)  # Remove mentions (@username)\n",
        "    comment = re.sub(r'#\\w+', '', comment)  # Remove hashtags\n",
        "    comment = re.sub(r'[^\\x00-\\x7F]+', '', comment)  # Remove emojis\n",
        "    comment = re.sub(r'[^\\w\\s]', '', comment)  # Remove punctuation\n",
        "\n",
        "    # Basic comment length and structure check (minimum characters for meaningful comment)\n",
        "    if len(comment) < 10 or len(comment.split()) < 3:\n",
        "        return None\n",
        "\n",
        "    # Check if the comment contains sarcasm (rudimentary rule-based sarcasm detection)\n",
        "    sarcasm_keywords = ['just kidding', 'lol', 'joking', 'not really', 'haha', 'sarcasm']\n",
        "    if any(sarcasm_word in comment for sarcasm_word in sarcasm_keywords):\n",
        "        return None\n",
        "\n",
        "    # If the comment is not empty or gibberish, return it\n",
        "    if len(comment.replace(\" \", \"\")) > 0:\n",
        "        return comment\n",
        "    return None\n",
        "\n",
        "# Function to classify sentiment using VADER lexicon and return numeric labels\n",
        "def lexicon_based_sentiment(comment):\n",
        "    sentiment_score = sia.polarity_scores(comment)\n",
        "    compound_score = sentiment_score['compound']\n",
        "    if compound_score >= 0.05:\n",
        "        return 1  # Positive sentiment\n",
        "    elif compound_score <= -0.05:\n",
        "        return 0  # Negative sentiment\n",
        "    else:\n",
        "        return 2  # Neutral sentiment\n",
        "\n",
        "# Function to detect sentiment for specific key points\n",
        "def detect_key_point_sentiment(comment, key_points):\n",
        "    sentiment_labels = {}\n",
        "    for key_point in key_points:\n",
        "        if key_point in comment:\n",
        "            sentiment_labels[key_point] = lexicon_based_sentiment(comment)\n",
        "        else:\n",
        "            sentiment_labels[key_point] = None  # No mention of the key point\n",
        "    return sentiment_labels\n",
        "\n",
        "# Function to generate meaningful fake comments related to mobile phones\n",
        "def generate_fake_phone_reviews(num_comments=100):\n",
        "    comments = []\n",
        "\n",
        "    # Example structure for reviews\n",
        "    reviews = [\n",
        "        \"The camera is outstanding, takes fantastic photos even in low light. The battery lasts a full day easily.\",\n",
        "        \"I’m so impressed with the performance of this phone, the display is sharp and vibrant. However, the price is a bit high.\",\n",
        "        \"The design feels premium, but the screen could be brighter. The build quality is solid, and the storage is more than enough.\",\n",
        "        \"The battery is terrible, it drains quickly. However, the performance is smooth, and the camera quality is decent.\",\n",
        "        \"Not happy with the audio quality, but the camera and performance are decent. Definitely not worth the high price.\",\n",
        "        \"The screen is amazing, but the camera doesn’t live up to expectations. Overall, it's a solid phone for the price.\",\n",
        "        \"The phone’s design is sleek, but the storage is not enough for my needs. Performance is good though.\",\n",
        "        \"Love the battery life, it lasts me throughout the day with moderate use. The camera could be better, but it's okay for the price.\",\n",
        "        \"I was impressed by the build quality and display. However, the performance under heavy use is a little sluggish.\",\n",
        "        \"The screen is very clear and bright, but the battery life is poor, especially when using high-performance apps.\"\n",
        "    ]\n",
        "\n",
        "    for _ in range(num_comments):\n",
        "        # Select a random review and add key points\n",
        "        review = random.choice(reviews)\n",
        "\n",
        "        # Randomly add or replace some key points\n",
        "        review = review + \" \" + random.choice(key_points)\n",
        "\n",
        "        comments.append(review)\n",
        "    return comments\n",
        "\n",
        "# Generate fake phone review comments\n",
        "comments = generate_fake_phone_reviews(100)\n",
        "\n",
        "# Define the output CSV file path\n",
        "output_file_path = 'sentiment_data_phone_reviews.csv'  # Update with your path\n",
        "\n",
        "# Write the sentiment data to CSV\n",
        "with open(output_file_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    fieldnames = ['comment', 'sentiment_label'] + key_points  # Include the key points as columns\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "    # Write header\n",
        "    writer.writeheader()\n",
        "\n",
        "    for comment in comments:\n",
        "        processed_comment = preprocess(comment)\n",
        "        if processed_comment:\n",
        "            # Get overall sentiment using lexicon\n",
        "            overall_sentiment = lexicon_based_sentiment(processed_comment)\n",
        "\n",
        "            # Get sentiment labels for each key point\n",
        "            key_point_sentiments = detect_key_point_sentiment(processed_comment, key_points)\n",
        "\n",
        "            # Merge overall sentiment with key point sentiment labels\n",
        "            row = {'comment': processed_comment, 'sentiment_label': overall_sentiment}\n",
        "            row.update(key_point_sentiments)  # Add the key point sentiments to the row\n",
        "\n",
        "            # Write the row to the CSV\n",
        "            writer.writerow(row)\n",
        "\n",
        "print(f\"Sentiment data stored in: {output_file_path}\")\n"
      ],
      "metadata": {
        "id": "t_uR4ouk2PR4",
        "outputId": "550c12f0-bd8e-4460-d905-ff4bc9df7c3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'faker'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-9cb5c95c01f3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentimentIntensityAnalyzer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfaker\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFaker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'faker'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import csv\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "# Your YouTube Data API Key (replace with your actual key)\n",
        "API_KEY = \"AIzaSyBtJjniAVMDaQN3OaocZpP7MyAVBgXPl2Q\"\n",
        "\n",
        "# Define key points related to the video comments\n",
        "key_points = ['camera', 'battery', 'performance', 'display']\n",
        "\n",
        "# Function to extract video ID from YouTube URL\n",
        "def extract_video_id(url):\n",
        "    match = re.search(r\"(?<=v=)[^&]+\", url)\n",
        "    return match.group(0) if match else None\n",
        "\n",
        "# Function to fetch comments from a YouTube video\n",
        "def get_youtube_comments(video_url, max_comments=50):\n",
        "    video_id = extract_video_id(video_url)\n",
        "    if not video_id:\n",
        "        print(\"Invalid YouTube URL\")\n",
        "        return []\n",
        "\n",
        "    youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
        "\n",
        "    comments = []\n",
        "    request = youtube.commentThreads().list(\n",
        "        part=\"snippet\",\n",
        "        videoId=video_id,\n",
        "        maxResults=max_comments,\n",
        "        textFormat=\"plainText\"\n",
        "    )\n",
        "\n",
        "    response = request.execute()\n",
        "\n",
        "    for item in response.get(\"items\", []):\n",
        "        comment_text = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
        "        processed_comment = preprocess(comment_text)\n",
        "        if processed_comment and contains_key_points(processed_comment, key_points):\n",
        "            comments.append(processed_comment)\n",
        "\n",
        "    return comments\n",
        "\n",
        "# Function to clean and preprocess comments\n",
        "def preprocess(comment):\n",
        "    comment = comment.lower().strip()\n",
        "    comment = re.sub(r'http\\S+', '', comment)  # Remove URLs\n",
        "    comment = re.sub(r'@\\w+', '', comment)  # Remove mentions\n",
        "    comment = re.sub(r'#\\w+', '', comment)  # Remove hashtags\n",
        "    comment = re.sub(r'[^\\x00-\\x7F]+', '', comment)  # Remove emojis\n",
        "    comment = re.sub(r'[^\\w\\s]', '', comment)  # Remove punctuation\n",
        "    return comment if len(comment.split()) > 2 else None  # Only keep comments with more than 2 words\n",
        "\n",
        "# Function to check if the comment contains any of the key points\n",
        "def contains_key_points(comment, key_points):\n",
        "    return any(key_point in comment for key_point in key_points)\n",
        "\n",
        "# Function to store comments in a CSV file\n",
        "def store_comments(video_url, output_file=\"youtube_comments.csv\"):\n",
        "    comments = get_youtube_comments(video_url)\n",
        "\n",
        "    if not comments:\n",
        "        print(\"No meaningful comments found.\")\n",
        "        return\n",
        "\n",
        "    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(['comment'])\n",
        "\n",
        "        for comment in comments:\n",
        "            writer.writerow([comment])\n",
        "\n",
        "    print(f\"Comments saved to {output_file}\")\n",
        "\n",
        "# Ask for YouTube video URL in terminal\n",
        "if __name__ == \"__main__\":\n",
        "    video_url = input(\"Please enter the YouTube video URL: \")  # Take YouTube URL from the user\n",
        "    store_comments(video_url)\n"
      ],
      "metadata": {
        "id": "NAOBubecAer2",
        "outputId": "9f02a975-7b15-4478-eb16-5228fcfd9a41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter the YouTube video URL: https://www.youtube.com/watch?v=SAb4zRyxrD4\n",
            "Comments saved to youtube_comments.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import csv\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "# Your YouTube Data API Key (replace with your actual key)\n",
        "API_KEY = \"AIzaSyBtJjniAVMDaQN3OaocZpP7MyAVBgXPl2Q\"\n",
        "\n",
        "# Function to extract video ID from YouTube URL\n",
        "def extract_video_id(url):\n",
        "    match = re.search(r\"(?<=v=)[^&]+\", url)\n",
        "    return match.group(0) if match else None\n",
        "\n",
        "# Function to fetch comments from a YouTube video\n",
        "def get_youtube_comments(video_url, max_comments=50):\n",
        "    video_id = extract_video_id(video_url)\n",
        "    if not video_id:\n",
        "        print(\"Invalid YouTube URL\")\n",
        "        return []\n",
        "\n",
        "    youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
        "\n",
        "    comments = []\n",
        "    request = youtube.commentThreads().list(\n",
        "        part=\"snippet\",\n",
        "        videoId=video_id,\n",
        "        maxResults=max_comments,\n",
        "        textFormat=\"plainText\"\n",
        "    )\n",
        "\n",
        "    response = request.execute()\n",
        "\n",
        "    for item in response.get(\"items\", []):\n",
        "        comment_text = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
        "        processed_comment = preprocess(comment_text)\n",
        "        if processed_comment:\n",
        "            comments.append(processed_comment)\n",
        "\n",
        "    return comments\n",
        "\n",
        "# Function to clean and preprocess comments\n",
        "def preprocess(comment):\n",
        "    comment = comment.lower().strip()\n",
        "    comment = re.sub(r'http\\S+', '', comment)  # Remove URLs\n",
        "    comment = re.sub(r'@\\w+', '', comment)  # Remove mentions\n",
        "    comment = re.sub(r'#\\w+', '', comment)  # Remove hashtags\n",
        "    comment = re.sub(r'[^\\x00-\\x7F]+', '', comment)  # Remove emojis\n",
        "    comment = re.sub(r'[^\\w\\s]', '', comment)  # Remove punctuation\n",
        "    return comment if len(comment.split()) > 2 else None  # Only keep comments with more than 2 words\n",
        "\n",
        "# Function to store comments in a CSV file\n",
        "def store_comments(video_url, output_file=\"youtube_ comments.csv\"):\n",
        "    comments = get_youtube_comments(video_url)\n",
        "\n",
        "    if not comments:\n",
        "        print(\"No meaningful comments found.\")\n",
        "        return\n",
        "\n",
        "    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(['comment'])\n",
        "\n",
        "        for comment in comments:\n",
        "            writer.writerow([comment])\n",
        "\n",
        "    print(f\"Comments saved to {output_file}\")\n",
        "\n",
        "# Ask for YouTube video URL in terminal\n",
        "if __name__ == \"__main__\":\n",
        "    video_url = input(\"Please enter the YouTube video URL: \")  # Take YouTube URL from the user\n",
        "    store_comments(video_url)\n"
      ],
      "metadata": {
        "id": "B_WmF7BDHkFn",
        "outputId": "9f71ce5d-0f45-439a-e3ed-456a23b82299",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter the YouTube video URL: https://www.youtube.com/watch?v=SAb4zRyxrD4\n",
            "Comments saved to youtube_comments.csv\n"
          ]
        }
      ]
    }
  ]
}